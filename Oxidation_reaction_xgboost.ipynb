{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893e24a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import DotProduct, WhiteKernel\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "from scipy.optimize import minimize, differential_evolution\n",
    "from scipy.stats import norm\n",
    "from sklearn.utils import resample\n",
    "from typing import Tuple, List, Optional, Dict, Any\n",
    "\n",
    "import optuna\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "29b7ef56",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('Oxidation_reaction.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda62005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate correlation\n",
    "def calculate_corr(data):\n",
    "    plt.rcParams['font.sans-serif'] = ['Times New Roman']\n",
    "    plt.rcParams['axes.unicode_minus']=False\n",
    "\n",
    "    corr1 = data.corr(method='pearson')\n",
    "    print(\"pearson\",corr1)\n",
    "    fig, ax1 = plt.subplots(1, 1, figsize=(8, 8))\n",
    "    sns.heatmap(corr1, ax=ax1, cmap=\"plasma\", annot=True)\n",
    "    ax1.set_title(\"Correlation (pearson) Heatmap of Variables in LHS Sampled Data\")\n",
    "\n",
    "    corr2 = data.corr(method='spearman')\n",
    "    print(\"spearman\",corr2)\n",
    "    fig, ax1 = plt.subplots(1, 1, figsize=(8, 8))\n",
    "    sns.heatmap(corr2, ax=ax1, cmap=\"plasma\", annot=True)\n",
    "    ax1.set_title(\"Correlation (spearman) Heatmap of Variables in LHS Sampled Data\")\n",
    "\n",
    "    corr3 = data.corr(method='kendall')\n",
    "    print(\"kendall\",corr3)\n",
    "    fig, ax1 = plt.subplots(1, 1, figsize=(8, 8))\n",
    "    sns.heatmap(corr3, ax=ax1, cmap=\"plasma\", annot=True)\n",
    "    ax1.set_title(\"Correlation (kendall) Heatmap of Variables in LHS Sampled Data\")\n",
    "calculate_corr(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8bb7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model filter\n",
    "def model_filter(data):\n",
    "    X=data.drop(['Residual of 17a','Yield'],axis=1)\n",
    "    y=data['Yield']\n",
    "    models=[xgb.XGBRegressor(objective='reg:squarederror'),\n",
    "       RandomForestRegressor(),\n",
    "        ExtraTreesRegressor(),\n",
    "        GradientBoostingRegressor(),\n",
    "        AdaBoostRegressor(),\n",
    "        DecisionTreeRegressor(),\n",
    "        GaussianProcessRegressor(kernel = DotProduct() + WhiteKernel())\n",
    "       ]\n",
    "    models_name=['XGBRegressor','RandomForestRegressor','ExtraTreesRegressor','GradientBoostingRegressor','AdaBoostRegressor','DecisionTreeRegressor','GaussianProcessRegressor']\n",
    "    all_rmse_test=[]\n",
    "    all_rmse_train=[]\n",
    "\n",
    "    for model in models:\n",
    "        kf = KFold(n_splits=4,shuffle=True, random_state=42)\n",
    "        rmse_test_list=[]\n",
    "        rmse_train_list=[]\n",
    "        for train_index, test_index in kf.split(X, y):\n",
    "            X_train=X.iloc[train_index]\n",
    "            y_train=y.iloc[train_index]\n",
    "            X_test=X.iloc[test_index]\n",
    "            y_test=y.iloc[test_index]\n",
    "            \n",
    "            model.fit(X_train,y_train)\n",
    "            y_pred_test = model.predict(X_test)\n",
    "            y_pred_train=model.predict(X_train)\n",
    "\n",
    "            rmse_test_list.append(np.sqrt(mean_squared_error(y_test, y_pred_test)))\n",
    "            rmse_train_list.append(np.sqrt(mean_squared_error(y_train, y_pred_train)))\n",
    "\n",
    "        all_rmse_test.append(np.mean(rmse_test_list))\n",
    "        all_rmse_train.append(np.mean(rmse_train_list))\n",
    "\n",
    "    print(\"rmse_test:\")\n",
    "    for i in range(len(models_name)):\n",
    "        print(f\"{models_name[i]}: {all_rmse_test[i]}\")\n",
    "    print(\"rmse_train:\")\n",
    "    for i in range(len(models_name)):\n",
    "        print(f\"{models_name[i]}: {all_rmse_train[i]}\")\n",
    "model_filter(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77f47d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=data.drop(['Residual of 17a','Yield'],axis=1)\n",
    "y=data['Yield']\n",
    "\n",
    "lower_rmse = np.inf\n",
    "lower_number = 0\n",
    "\n",
    "def model_optimize():\n",
    "    def objective(trial):\n",
    "\n",
    "        params = {\n",
    "            'objective': 'reg:squarederror', \n",
    "            'booster': 'gbtree', \n",
    "            'random_state': 42,\n",
    "            \n",
    "            'n_estimators': trial.suggest_int('n_estimators', 100, 3000, step=100),\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 9),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "            'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "            \n",
    "            'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),\n",
    "            'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),\n",
    "        }\n",
    "\n",
    "        model = xgb.XGBRegressor(**params)\n",
    "\n",
    "        kfold = KFold(n_splits=4, shuffle=True, random_state=42)\n",
    "        rmse_scores = []\n",
    "        for train_index, test_index in kfold.split(X, y):\n",
    "            X_train, y_train = X.iloc[train_index], y.iloc[train_index]\n",
    "            X_test, y_test = X.iloc[test_index], y.iloc[test_index]\n",
    "            model = xgb.XGBRegressor(**params)\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "            y_pred_test = model.predict(X_test)\n",
    "            rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "            rmse_scores.append(rmse)\n",
    "        \n",
    "        avg_rmse = np.mean(rmse_scores)\n",
    "        \n",
    "        return avg_rmse\n",
    "\n",
    "    class EarlyStopCallback:\n",
    "        def __init__(self, patience, min_delta):\n",
    "            self.patience = patience\n",
    "            self.min_delta = min_delta\n",
    "            self.best_score = None\n",
    "            self.wait = 0\n",
    "            self.stop_now = False\n",
    "\n",
    "        def __call__(self, study, trial):\n",
    "            current_score = study.best_value\n",
    "            \n",
    "            if self.best_score is None:\n",
    "                self.best_score = current_score\n",
    "            else:\n",
    "                \n",
    "                if abs(current_score - self.best_score)/self.best_score > self.min_delta:\n",
    "                    self.best_score = current_score\n",
    "                    self.wait = 0\n",
    "                else:\n",
    "                    self.wait += 1\n",
    "                \n",
    "\n",
    "                if self.wait >= self.patience:\n",
    "                    self.stop_now = True\n",
    "                    study.stop()\n",
    "\n",
    "    early_stopping_callback = EarlyStopCallback(patience=50, min_delta=0.01)\n",
    "    \n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(objective, n_trials=500, callbacks=[early_stopping_callback], show_progress_bar=True)\n",
    "    \n",
    "\n",
    "    best_params = study.best_params\n",
    "    best_score = study.best_value \n",
    "    \n",
    "    final_best_params = {\n",
    "        'objective': 'reg:squarederror',\n",
    "        'booster': 'gbtree'\n",
    "    }\n",
    "    final_best_params.update(best_params)\n",
    "\n",
    "    print(\"best_param:\", final_best_params)\n",
    "    print(\"best_score:\", best_score)\n",
    "\n",
    "    return final_best_params, best_score\n",
    "params, score = model_optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb5f6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class XGBoostBayesianOptimizer:\n",
    "    def __init__(self, \n",
    "                 X_train: np.ndarray, \n",
    "                 y_train: np.ndarray, \n",
    "                 xgb_params: Dict[str, Any],\n",
    "                 n_estimators: int = 10, \n",
    "                 random_state: int = 42):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.xgb_params = xgb_params\n",
    "        self.n_estimators = n_estimators\n",
    "        self.random_state = random_state\n",
    "        self.models = []\n",
    "        \n",
    "        self.best_y = np.max(y_train) \n",
    "        \n",
    "        self._train_bootstrap_models()\n",
    "\n",
    "    def _train_bootstrap_models(self):\n",
    "        self.models = []\n",
    "        \n",
    "        for i in range(self.n_estimators):\n",
    "            X_sample, y_sample = resample(self.X_train, self.y_train, \n",
    "                                          replace=True, \n",
    "                                          n_samples=len(self.X_train), \n",
    "                                          random_state=range(self.n_estimators)[i])\n",
    "            \n",
    "            model = xgb.XGBRegressor(**self.xgb_params, random_state=range(self.n_estimators)[i])\n",
    "            model.fit(X_sample, y_sample, eval_set=[(X_sample, y_sample)], verbose=False)\n",
    "            \n",
    "            self.models.append(model)\n",
    "        \n",
    "        print(f\"finished {len(self.models)} Bootstrap models\")\n",
    "\n",
    "    def predict_with_uncertainty(self, X: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        X = np.atleast_2d(X)\n",
    "        predictions = np.array([model.predict(X) for model in self.models])\n",
    "        \n",
    "        mean = np.mean(predictions, axis=0)\n",
    "        std = np.std(predictions, axis=0)\n",
    "        \n",
    "        std = np.maximum(std, 1e-6)\n",
    "        \n",
    "        return mean, std\n",
    "\n",
    "    def _acquisition_function_ei(self, X: np.ndarray, xi: float = 0.01) -> np.ndarray:\n",
    "        X = np.atleast_2d(X)\n",
    "        mu, sigma = self.predict_with_uncertainty(X)\n",
    "        \n",
    "        mu = mu.flatten()\n",
    "        sigma = sigma.flatten()\n",
    "        \n",
    "        with np.errstate(divide='ignore'):\n",
    "            imp = mu - self.best_y - xi\n",
    "            Z = imp / sigma\n",
    "            \n",
    "            ei = imp * norm.cdf(Z) + sigma * norm.pdf(Z)\n",
    "            ei[sigma == 0.0] = 0.0\n",
    "\n",
    "        print(mu,sigma,ei)\n",
    "\n",
    "        return -ei\n",
    "\n",
    "    def suggest_next_sample(self, \n",
    "                            bounds: List[Tuple[float, float]], \n",
    "                            n_restarts: int = 20, \n",
    "                            xi: float = 0.01) -> np.ndarray:\n",
    "\n",
    "        bounds = np.array(bounds)\n",
    "        n_dims = len(bounds)\n",
    "        \n",
    "        best_x = None\n",
    "        max_ei_value = -np.inf\n",
    "        \n",
    "        for i in range(n_restarts):\n",
    "            x0 = np.random.uniform(bounds[:, 0], bounds[:, 1])\n",
    "            \n",
    "            def objective(x):\n",
    "                return self._acquisition_function_ei(x.reshape(1, -1), xi)[0]\n",
    "            \n",
    "            try:\n",
    "                res = minimize(\n",
    "                    fun=objective,\n",
    "                    x0=x0,\n",
    "                    bounds=bounds,\n",
    "                    method='L-BFGS-B',\n",
    "                    options={'ftol': 1e-6, 'maxiter': 200}\n",
    "                )\n",
    "                \n",
    "                if res.success and res.fun < max_ei_value: \n",
    "                    pass \n",
    "                    \n",
    "            except Exception as e:\n",
    "                continue\n",
    "                \n",
    "        candidates = []\n",
    "        for i in range(n_restarts):\n",
    "            x0 = np.random.uniform(bounds[:, 0], bounds[:, 1])\n",
    "            def objective(x):\n",
    "                return self._acquisition_function_ei(x.reshape(1, -1), xi)[0]\n",
    "            \n",
    "            res = minimize(objective, x0, bounds=bounds, method='L-BFGS-B', options={'ftol': 1e-6})\n",
    "            if res.success:\n",
    "                candidates.append((res.x, -res.fun))\n",
    "        \n",
    "        print(\"All sampled data:\",candidates)\n",
    "        candidates.sort(key=lambda k: k[1], reverse=True)\n",
    "        best_x_1, best_ei_1 = candidates[0]\n",
    "        best_x_2, best_ei_2 = candidates[1]\n",
    "        print(f\"New sample point 1. (EI={best_ei_1:.4f}): {best_x_1}\")\n",
    "        print(f\"New sample point 2. (EI={best_ei_2:.4f}): {best_x_2}\")\n",
    "\n",
    "\n",
    "my_xgb_params = params\n",
    "\n",
    "optimizer = XGBoostBayesianOptimizer(\n",
    "    X_train=X, \n",
    "    y_train=y, \n",
    "    xgb_params=my_xgb_params,\n",
    "    n_estimators=10  \n",
    ")\n",
    "\n",
    "\n",
    "search_bounds = [(0.5, 1.75),(1,1.75),(0.8,2.0),(1.1,1.5),(70,90),(10,60)] \n",
    "\n",
    "optimizer.suggest_next_sample(\n",
    "    bounds=search_bounds, \n",
    "    n_restarts=20,\n",
    "    xi=0.01        \n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
